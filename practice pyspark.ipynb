{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bce7d5ff-37b2-4c55-baf7-768c74d6453a",
     "showTitle": true,
     "title": "pyspark dataframe"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b33b7d9f-da30-4913-9ec6-ddc92cd7b8a6",
     "showTitle": false,
     "title": "create and empty dataframe"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmptyRDD[0] at emptyRDD at NativeMethodAccessorImpl.java:0\n"
     ]
    }
   ],
   "source": [
    "#create an empty rdd\n",
    "rdd1=spark.sparkContext.emptyRDD()\n",
    "print(rdd1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6c54af1-7430-4cc2-89fd-10bd6be09d91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[1] at readRDDFromInputStream at PythonRDD.scala:435\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "#create empty rdd using parallelize\n",
    "rdd2=spark.sparkContext.parallelize([])\n",
    "print(rdd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b89f3e76-0f9b-4e6f-b6be-19fdc7b3a509",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "#create schema\n",
    "schema=StructType([StructField('firstname',StringType(),True),\n",
    "                   StructField('middlename',StringType(),True),\n",
    "                   StructField('lastname',StringType(),True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ea16190-044c-4b0b-a48f-1aacfc0e4884",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+\n|firstname|middlename|lastname|\n+---------+----------+--------+\n+---------+----------+--------+\n\nroot\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#create an empty dataframe from emptyrdd\n",
    "df1=spark.createDataFrame(rdd2,schema)\n",
    "df1.show()\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5494e161-3c36-488c-b344-d69d4a265c6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+\n|firstname|middlename|lastname|\n+---------+----------+--------+\n+---------+----------+--------+\n\nroot\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#convert empty rdd to a dataframe \n",
    "df2=rdd1.toDF(schema)\n",
    "df2.show()\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b31c0439-5421-4836-9643-9f79a4a86a69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+\n|firstname|middlename|lastname|\n+---------+----------+--------+\n+---------+----------+--------+\n\nroot\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#crate an empty dataframe directly \n",
    "df3=spark.createDataFrame([],schema)\n",
    "df3.show()\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87bb039e-33c9-4540-95b4-6237e7fe57e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n||\n++\n++\n\nroot\n\n"
     ]
    }
   ],
   "source": [
    "#create an empty dataframe without schema\n",
    "df4=spark.createDataFrame([],StructType([]))\n",
    "df4.show()\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c39e43ef-98a6-4b3a-a5bd-b04cdd2a0610",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "practice pyspark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
